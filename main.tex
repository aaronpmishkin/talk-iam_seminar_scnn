\documentclass[usenames,dvipsnames,mathserif,notheorems]{beamer}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{centernot}

\input{macros/math}
\input{macros/plots}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.3cm, fill=black, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.3cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=black]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Two-Layer ReLU Networks:}
\subtitle{New Insights from Convex Reformulations}
\author{Aaron Mishkin \and Arda Sahiner \and Mert Pilanci}
%\institute{Stanford University}
\collaborators{
	\includegraphics[width=0.2\linewidth]{assets/rainy.jpg}
	\includegraphics[width=0.2\linewidth]{assets/arda.jpg}
	\includegraphics[width=0.2\linewidth]{assets/mert.jpg}
}

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

\newcommand{\horizontalrule}{
	{
			\vspace{-0.5em}
			\center \rule{\textwidth}{0.1em}
			\vspace{-0.2em}
		}
}

\newcommand{\red}[1]{\textcolor{Red}{#1}}
\newcommand{\green}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\blue}[1]{\textcolor{DarkBlue}{#1}}
\newcommand{\purple}[1]{\textcolor{Magenta}{#1}}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\date{}

\begin{document}

\maketitle
%% main content starts %%

\begin{frame}{Overview}

	{
		\large \red{Problem}: Non-convex neural networks are hard to analyze.
	}

	\pause
	\vspace{0.5em}
	\horizontalrule
	\vspace{0.5em}

	{
		\large \green{Our Contribution}: study neural networks through convex reformulations.
	}

	\pause
	\vspace{0.5em}

	\begin{enumerate}
		\item \textbf{Optima}: characterize all critical points of the non-convex training objective.\pause

		\item \textbf{Cone Decompositions}: develop new connections between convex training programs.\pause

		\item \textbf{Algorithms}: leverage our theory for robust, tuning-free, and fast algorithms.
	\end{enumerate}

\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge I. 10 Years of Neural Nets
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Context: Ten Years since AlexNet}

	{
		\large \textbf{10 Years Ago}: AlexNet won ILSVRC 2012 and started the modern ``deep learning'' movement in ML.
	}

	\pause
	\vspace{1ex}

	\begin{itemize}
		\large
		\item AlexNet won with \( \red{84.69}\% \) top-five accuracy \citep{krizhevsky2012alexnet}.
		      \vspace{1ex}

		\item Today, models get \( \green{99.02}\% \) top-5 accuracy \citep{yuan2021florence}!
		      \vspace{1ex}

		      \pause
		\item Using all sorts of tricks like pre-training, transformers, etc\ldots
	\end{itemize}

	\pause
	\horizontalrule

	\begin{center}
		\Large
		Huge progress in terms of downstream benchmarks!
	\end{center}

\end{frame}


\begin{frame}{Context: DALL$\cdot$E 2}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.65\linewidth]{assets/bowl_of_soup.jpg}
		\caption*{Generated by DALL$\cdot$E 2}%
	\end{figure}

	\begin{center}
		\textit{\Large A bowl of soup that is a portal to another
			dimension as digital art.}
	\end{center}

	\source{https://openai.com/dall-e-2/}

\end{frame}

\begin{frame}{Context: Cost of Training DALL$\cdot$E 2}

	\begin{center}
		\Large
		DALL$\cdot$E 2 has 5.5 billion parameters and took \red{billions} of Adam
		iterations to fit \citep{ramesh2022dalle}.
	\end{center}

	\pause
	\horizontalrule

	\begin{center}
		\Large
		\textbf{Main Challenge}: neural networks are \textcolor{red}{non-convex}.
	\end{center}


	\begin{figure}[]
		\centering
		\input{assets/non_convex.tex}
	\end{figure}

\end{frame}

\begin{frame}{Context: Challenges from Non-Convexity}
	\begin{center}
		\Large
		Non-convexity makes analysis and training \red{hard}.
	\end{center}

	\pause
	\begin{itemize}
		\item \textbf{Optimality Conditions}: Stationarity \( \centernot \implies \) optimality.
		      Few/no certificates.

		      \pause
		\item \textbf{Model Churn}: different local/global minima,
		      have different performance at test time~\citep{henderson2018deep}.

		      \pause
		\item \textbf{Tuning}: step-size, momentum, batch-size, etc.
	\end{itemize}

	\pause
	\horizontalrule

	\vspace{-2ex}

	\begin{center}
		\Large
		But these issues don't exist for \green{convex} models!
	\end{center}

	%\begin{itemize}
	%    \item \textbf{Certificates}: stationary points are global minima.

	%          \pause
	%    \item \textbf{Model Churn}: strict/strong convexity gives uniqueness.

	%          \pause
	%    \item \textbf{Tuning}: line-search, full-batch methods, acceleration, etc.
	%\end{itemize}

\end{frame}

\begin{frame}{Context: Practical Challenges}

	\begin{center}
		\large
		Recovering a two-layer ReLU network from data generated by a two-layer ReLU network.
	\end{center}

	\pause

	\begin{figure}[]
		\centering
		\includegraphics[width=0.9\textwidth]{assets/synthetic_classification.png}
	\end{figure}
\end{frame}

\begin{frame}{Context: Better Methods}
	\begin{center}
		{ \huge
			We need better methods!
		}

	\end{center}
\end{frame}


\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge II. Equivalent (Convex) Model Classes
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Convex Reformulations: Flavor of Results}
	\large

	\textbf{Basic Idea}: We start with a \red{non-convex} optimization problem and derive
	an equivalent \green{convex} program.

	\pause
	\vspace{2em}

	\textbf{Equivalent} means:
	\vspace{0.5em}
	\begin{itemize}
		\item The global minima have the same values: \( p^* = d^* \)
		      \vspace{0.5em}
		\item We can map a solution \( u^* \) for one problem into a solution
		      \( v^* \) for the other.
		      \vspace{0.5em}
	\end{itemize}

\end{frame}


\begin{frame}{Convex Reformulations: Two-Layer ReLU Networks}

	{\large \textcolor{Red}{Non-Convex Problem}}
	\[
		\min_{W_1, w_2} \underbrace{\half \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2}_{\text{Squared Error}}
		+ \underbrace{\lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + |w_{2j}|^2}_{\text{Weight Decay}},
	\]
	where \( \rbr{x}_+ = \max\cbr{x, 0} \) is the ReLU activation.
	\pause

	\begin{figure}[]
		\centering
		\input{assets/neural_net}
	\end{figure}

\end{frame}


\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\pause

	\begin{figure}[]
		\centering
		\input{assets/activation_pattern_0}
	\end{figure}

	\phantom{
		\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)
	}

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\begin{figure}[]
		\centering
		\input{assets/activation_pattern_1}
	\end{figure}

	\pause
	\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\begin{figure}[]
		\centering
		\input{assets/activation_pattern_2}
	\end{figure}


	\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)

\end{frame}


\begin{frame}{Convex Reformulations: Convex Problem}

	{\large \textcolor{ForestGreen}{Convex Reformulation}} \citep{pilanci2020convex}
	\[
		\begin{aligned}
			\min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2           \\
			         & \hspace{0.2em} \text{s.t. }
			v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},
		\end{aligned}
	\]
	where \( D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] \).
	\pause

	\begin{figure}[]
		\centering
		\input{assets/convex_reformulation}
	\end{figure}
\end{frame}

\begin{frame}{Convex Reformulations: Breaking it Down}

	\[
		\begin{aligned}
			\min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2           \\
			         & \hspace{0.2em} \text{s.t. }
			v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},
		\end{aligned}
	\]
	where \( \purple{D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)]} \).

	\horizontalrule

	\begin{itemize}
		\item \purple{\( D_j \) is a ReLU activation pattern induced by ``gate'' \( g_j \).}
		      \pause
		      \begin{itemize}
			      \item \([D_j]_{ii} = 1\) if \( \abr{x_i, g_i} \geq 0 \) and \( 0 \) otherwise.
		      \end{itemize}

	\end{itemize}
	\vspace{7.3em}
\end{frame}


\begin{frame}{Convex Reformulations: Breaking it Down}

	\[
		\begin{aligned}
			\min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2           \\
			         & \hspace{0.2em} \purple{\text{s.t. }
				v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},}
		\end{aligned}
	\]
	where \( D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] \).

	\horizontalrule

	\begin{itemize}
		\item \( D_j \) is a ReLU activation pattern induced by ``gate'' \( g_j \).
		      \begin{itemize}
			      \item \([D_j]_{ii} = 1\) if \( \abr{x_i, g_i} \geq 0 \) and \( 0 \) otherwise.
		      \end{itemize}
		\item \purple{The constraint \( v_j \in \calK_j \) implies
			      \[
				      \rbr{X v_j}_+ = D_j X v_j.
			      \]
			      That is, \( v_j \) has the activation encoded by \( D_j \).
		      }
	\end{itemize}
\end{frame}

\begin{frame}{Convex Reformulations: Breaking it Down}

	\[
		\begin{aligned}
			\min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\purple{\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2}  \\
			         & \hspace{0.2em} \text{s.t. }
			v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0}
		\end{aligned}
	\]
	where \( D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] \).

	\horizontalrule

	\begin{itemize}
		\item \( D_j \) is a ReLU activation pattern induced by ``gate'' \( g_j \).
		      \begin{itemize}
			      \item \([D_j]_{ii} = 1\) if \( \abr{x_i, g_i} \geq 0 \) and \( 0 \) otherwise.
		      \end{itemize}
		\item The constraint \( v_j \in \calK_j \) implies
		      \[
			      \rbr{X v_j}_+ = D_j X v_j.
		      \]
		      That is, \( v_j \) has the activation encoded by \( D_j \).
		\item \purple{Weight decay regularization turns into ``group \( \ell_1 \)'' penalty.}
	\end{itemize}
	\vspace{6em}
\end{frame}


\begin{frame}{Convex Reformulations: Solution Mapping}

	Given \( \rbr{v^*, w^*} \), an optimal non-convex ReLU network is obtained by

	\begin{equation*}
		\textbf{C to NC:} \quad \quad
		\begin{aligned}
			W_{1i} & = v_i^*/ \sqrt{\norm{v_i^*}}, \quad w_{2i} = \sqrt{\norm{v_i^*}}
			\\
			W_{1j} & = w_i^*/ \sqrt{\norm{w_i^*}}, \quad w_{2j} = -\sqrt{\norm{w_i^*}}.
		\end{aligned}
	\end{equation*}

	\pause
	\begin{itemize}
		\item Optimal solution \green{balances} weight between layers.
	\end{itemize}


	\pause
	\horizontalrule

	Given \( (W_{1i}^*, w_{2i}^*) \), an optimal convex ReLU model is
	\begin{equation*}
		\textbf{NC to C:} \quad \quad
		\begin{aligned}
			v_i & = W_{1i}^* \abs{w_{2i}^*} & \mbox{if \( w_{2i}^* \geq 0 \)} \\
			w_i & = W_{1i}^* \abs{w_{2i}^*} & \mbox{otherwise.}
		\end{aligned}
	\end{equation*}

	\pause
	\begin{itemize}
		\item Optimal solution \green{combines} weights from both layers.
	\end{itemize}

\end{frame}

\begin{frame}{Convex Reformulations: Hardness}


	\[
		p = \abs{\cbr{D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] : g_j \in \R^d }}
	\]

	\vspace{2em}
	\pause

	The \textbf{convex program} is:
	\vspace{0.5em}
	\begin{itemize}
		\item \red{Exponential in general}: \( p \in O(r \cdot (\frac{n}{r})^r) \),
		      where \( r = \text{rank}(X) \).
		      \vspace{0.25em}
		      \begin{itemize}
			      \item Bound comes from theory of hyperplane arrangements \citep{winder1966partitions}.
		      \end{itemize}
		      \pause

		      \vspace{0.5em}

		\item Highly \green{structured} --- it's a (constrained) generalized linear model!
	\end{itemize}

	\vspace{1em}
	\pause

	\begin{center}
		\Large
		We exchange one kind of hardness for another.
	\end{center}

\end{frame}


\begin{frame}{Convex Reformulations: Gated ReLU Networks}
	What about simpler convex reformulations?

	\[
		\begin{aligned}
			\textbf{C-ReLU}: \min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2                            \\
			                          & \hspace{0.2em} \red{\text{s.t. }
				v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},}
		\end{aligned}
	\]

	\pause
	\horizontalrule

	\textbf{Relaxation}: drop the cone constraints and simplify to obtain,
	\[
		\begin{aligned}
			\textbf{C-GReLU}: \min_{u} & \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{u_j}_2                                    \\
		\end{aligned}
	\]

	\pause
	\red{What does it mean? Is it still a neural network?}
\end{frame}


\begin{frame}{Convex Reformulations: Gated ReLU Networks}

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
		\textbf{Theorem 2.2} (informal): C-GReLU is equivalent to solving
		\[
			\min_{W_1, w_2} \half \norm{\sum_{j=1}^p \phi_{g_j}(X, W_{1j})w_{2j} - y}_2^2 + \frac{\lambda}{2} \sum_{j=1}^p \norm{W_{1j}}_2^2 + |w_{2j}|^2,
		\]
		with the ``Gated ReLU'' \citep{fiat2019decoupling} activation function
		\[ \phi_{g}(X, u) = \text{diag}(\mathbbm{1}(Xg \geq 0)) X u, \]
		and gate vectors \( g_j \) such that
		\[
			D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)].
		\]
	\end{beamercolorbox}
	\pause

	\textbf{Interpretation}: if \( u_j \not \in \calK_j \), then the activation
	must be decoupled from the linear mapping in the non-convex model.

\end{frame}



\begin{frame}{Gated ReLu Networks: Big Picture}
	\begin{center}
		\Large Are these models really different?
	\end{center}

	\pause

	\begin{figure}[]
		\centering
		\input{assets/relations_complete}
	\end{figure}
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge III. Solution Sets
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Model Problem: Constrained Group Lasso}

	{
		\large
		\textbf{General Approach}:
		\begin{enumerate}
			\large
			\item \pause
			      Characterize properties of \green{convex reformulation};
			      \vspace{1ex}
			\item \pause
			      Extend results to \red{non-convex} training problem
			      using solution mapping;
			      \pause
			\item \ldots \textbf{Profit} \ldots
		\end{enumerate}
	}

	\pause
	\horizontalrule

	\begin{itemize}
		\item Let \( Z \in \R^{n \times d} \) be a data matrix and \( y \) associated targets.
		      \pause
		\item Let \( \calB = \cbr{b_1, \ldots, b_p} \) partition the
		      feature set \( \cbr{1, \ldots, d} \).
		      \pause
	\end{itemize}

	\vspace{2ex}

	C-ReLU and C-GReLU are \green{constrained group lasso} problems:
	\vspace{-1ex}
	\begin{equation*}
		\textbf{CGL}: \min_{w}
		\cbr{ \half \norm{Z w - y}_2^2
			+ \lambda \sum_{\bi \in \calB} \norm{\wi}_2
			\, : \, \Ki^\top \wi \leq 0 \, \forall \, \bi \in \calB}.
	\end{equation*}

\end{frame}


\begin{frame}{CGL: Mapping to Convex Reformulations}

	\begin{equation*}
		\textbf{CGL}: \min_{w}
		\cbr{ \half \norm{\green{Z} w - y}_2^2
			+ \lambda \sum_{\red{\bi \in \calB}} \norm{\wi}_2
			\, : \, \blue{\Ki^\top \wi \leq 0} \, \forall \, \bi \in \calB}.
	\end{equation*}

	\vspace{3ex}

	\pause
	\horizontalrule

	\begin{itemize}
		\item \green{Basis expansion} \( Z = [D_1 X, \ldots, D_p X] \);
		      \pause
		      \vspace{1ex}
		\item \( \bi \in \calB \) corresponds to one \red{neuron/activation pattern} \( D_i \);
		      \pause
		      \vspace{1ex}
		\item C-ReLU: enforce \blue{cone constraints} with \( \Ki = X^\top (2 D_i - I) \);
		      \pause
		      \vspace{1ex}
		\item C-GReLU: \blue{unconstrained}, so \( \Ki = 0 \).
	\end{itemize}

\end{frame}

\begin{frame}{CGL: Optimality Conditions}
	\[
		L(W_1, w_2) := \norm{\sum_{j=1}^m (X W_{1j})_+ w_{wj} - y}_2^2
		+ \lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + |w_{2j}|^2
	\]

	\pause

	\textbf{Suppose}: \( \nabla L(W_1, w_2) = 0 \). Then\ldots
	\pause \red{critical point}.

	\pause
	\horizontalrule

	\textbf{CGL}: \green{KKT conditions} are necessary and sufficient for optimality:\pause

	\vspace{1ex}
	\begin{itemize}
		\item Primal-dual feasibility, complementary slackness.
		      \pause
		      \vspace{1ex}
		\item Stationary Lagrangian:
		      \[
			      \underbrace{Z_\bi^\top (Z w - y) - \Ki \ri}_{\vi} \in \partial \lambda \norm{\wi}_2.
		      \]
		      \pause
		      \begin{itemize}
			      \normalsize
			      \item It turns out each \( \vi \) is \textbf{unique}!
		      \end{itemize}
	\end{itemize}

\end{frame}

%\begin{frame}{CGL: Towards Characterizing the Optimal Set}

%    \textbf{Facts}: let \( (w, \rho) \) be primal dual optimal. \pause
%    \begin{itemize}
%        \item Model fit \( \hat y = Z w \) is \blue{constant} over optimal set \( \solfn(\lambda) \). \pause
%        \item Implies correlation \( \Xbi^\top (y - Z w) \) is \blue{constant} over \( \solfn(\lambda) \). \pause
%        \item We may assume \( \rho \) is \blue{unique} (e.g. min-norm dual solution).
%    \end{itemize}

%    \pause
%    \horizontalrule

%    \textbf{Non-zero Blocks}:
%    \begin{itemize}
%        \item Suppose \( \wi \neq 0 \).
%              \pause
%        \item Then \( s_\bi = \lambda \wi / \norm{\wi}_2 \).
%              \pause
%        \item Rearranging stationarity implies \( \exists \alpha_\bi > 0 \):
%              \[
%                  \wi = \blue{\alpha_\bi} \underbrace{\sbr{\Xbi^\top (y - Z w) - \Ki \ri}}_{\vi}.
%              \]
%              \pause
%        \item Every solution is non-negative scalings of these \( \vi \) vectors.
%    \end{itemize}

%\end{frame}

\begin{frame}{CGL: the Optimal Set}
	\vspace{-2ex}
	\begin{align*}
		\text{Let} \quad \hat y = Z w^*, \quad \quad \vi = Z_\bi^\top (Z w^* - y) - \Ki \ri^*
	\end{align*}
	\vspace{-2ex}
	\pause
	\horizontalrule
	\vspace{-1ex}

	Let \( \calS_\lambda \) be the set of blocks supported by some solution:
	\[
		\calS_\lambda = \cbr{\bi \in \calB : \exists w \in \solfn(\lambda), \wi \neq 0}.
	\]

	\pause

	\begin{proposition}[Informal]
		Fix \( \lambda > 0 \).
		The optimal set for CGL problem is
		given by
		\begin{equation*}
			\begin{aligned}
				\solfn(\lambda) =
				\big\{  w  \in  \R^d : \, & Z w = \hat y,                                     \\
				                          & \forall \, \bi  \in  \calS_\lambda,
				\wi =  \alpha_\bi \vi, \alpha_\bi \geq 0, \,                                  \\
				                          & \forall \, b_j \in \calB \setminus \calS_\lambda,
				\w_{b_j} = 0
				\big\}
			\end{aligned}
		\end{equation*}

	\end{proposition}

	\pause
	\begin{itemize}
		\item We can make \( S_\lambda \) explicit at the cost of
		      \red{additional linear constraints}.
	\end{itemize}

\end{frame}

\begin{frame}{CGL: Mapping back to Non-Convex Networks}

	Remember how to convert between solutions:
	\begin{equation*}
		\textbf{C to NC:} \quad \quad
		\begin{aligned}
			W_{1i} & = v_i^*/ \sqrt{\norm{v_i^*}}, \quad w_{2i} = \sqrt{\norm{v_i^*}}
			\\
			W_{1j} & = w_i^*/ \sqrt{\norm{w_i^*}}, \quad w_{2j} = -\sqrt{\norm{w_i^*}}.
		\end{aligned}
	\end{equation*}

	\pause
	\horizontalrule

	\begin{corollary}[Informal]
		Suppose \( m \geq m^* \).
		Then the optimal set for the ReLU problem is
		\vspace{-1ex}
		\begin{equation*}
			\begin{aligned}
				\hspace{-0.5em} \calO_\lambda  = \,
				\big\{
				 & (W_1,  w_2) :
				\, f_{W_1, w_2}(X)  =  \hat y,                        \\
				 & \forall \, \bi  \in  \calS_\lambda,
				W_{1i} = (\sfrac{\alpha_{i}}{\lambda})^{\sfrac{1}{2}} v_i,
				w_{2i} = (\alpha_i \lambda)^{\sfrac{1}{2}},
				\alpha_i \geq 0                                       \\
				 & \forall \, \bi  \in \calB \setminus \calS_\lambda,
				W_{1i} = 0, \, w_{2i} = 0
				\big\}.
			\end{aligned}
		\end{equation*}
	\end{corollary}

\end{frame}

\begin{frame}{CGL: Appearance of Solution Sets}
	\begin{figure}[]
		\centering
		\includegraphics[width=0.96\textwidth]{assets/solution_sets_vis_270.png}
	\end{figure}

	\begin{itemize}
		\item Non-convex parameterization maps polytope into curved
		      manifold.
	\end{itemize}


\end{frame}


\begin{frame}{CGL: Implications}
	\begin{center}
		\Large
		\red{What can we do with this theory?}
	\end{center}

	\pause
	\horizontalrule

	\begin{center}
		\Large Both \green{analysis} and \green{algorithms}!
	\end{center}

	\pause
	\begin{itemize}
		\large
		\item Conditions for \textbf{uniqueness},
		      \textbf{continuity}, \textbf{stability}, and more!

		      \pause
		      \vspace{1ex}
		\item Simple algorithms for computing \textbf{min-norm} solutions!
		      \pause
		      \vspace{1ex}

		\item \textbf{Optimal} (i.e. objective-preserving) methods for neuron pruning.
	\end{itemize}

\end{frame}


\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
	\begin{center}
		\huge IV. Cone Decompositions
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Cone Decompositions: Big Picture}
	\begin{center}
		\Large Are these models really different?
	\end{center}

	\begin{figure}[]
		\centering
		\input{assets/relations_complete}
	\end{figure}
\end{frame}

\begin{frame}{Cone Decompositions: Gated ReLU Networks }

	\begin{center}
		\textbf{Question}: when are Gated ReLU and ReLU networks equivalent?
	\end{center}

	\pause
	\horizontalrule

	Consider special case where \( \lambda = 0 \).

	\[
		\textbf{C-GReLU}: \min_{u} \norm{\sum_{j=1}^p D_j X u_j - y}_2^2. \hspace{11em}
	\]
	\vspace{-1em}

	\begin{center}
		\Large \red{V.S.}
	\end{center}

	\vspace{-2em}
	\[
		\begin{aligned}
			\textbf{C-ReLU}: \min_{u} & \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2. \\
			                          & \hspace{0.2em} \text{s.t. }
			v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},
		\end{aligned}
	\]

\end{frame}

\begin{frame}{Cone Decompositions: Equivalent Statement}

	\red{Equiv. Question}: when does \( u_j = v_j - w_j \) for some \( v_j, w_j \in \calK_j \)?

	\pause
	\vspace{1em}

	\green{Answer}: when \( \calK_j - \calK_j = \R^d \) and a ``cone decomposition'' exists.
	\pause

	\begin{figure}[]
		\centering
		\input{assets/cone_decomp}
	\end{figure}

\end{frame}
\begin{frame}{Cone Decomposition: Basic Result}

	\textbf{Recall}: \( \calK_j = \cbr{w : (2 D_j - I) X w \geq 0} \).
	\pause

	\begin{itemize}
		\item This is a polyhedral cone which we rewrite as
		      \[
			      \calK_j = \bigcap_{i=1}^n \cbr{w : [S_j]_{ii} \cdot \abr{x_i, w} \geq 0},
		      \]
		      where \( S_j = (2D_j - I) \).
	\end{itemize}

	\pause
	\horizontalrule

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
		\textbf{Proposition 3.1} (informal): If \( X \) is full row-rank,
		then \( \text{aff}(\calK_j) = \R^d \) and
		\( \calK_j - \calK_j = \R^d \).
	\end{beamercolorbox}

	\vspace{1em}

	\pause
	\begin{itemize}
		\item Unfortunately, there is \red{no extension} to full-rank \( X \).
	\end{itemize}

\end{frame}

\begin{frame}{Cone Decompositions: Not All Cones are Equal}

	\textbf{Alternative Idea}: show we don't need ``singular'' cones \( \calK_j \),
	\[
		\calK_j - \calK_j \subsetneq \R^d.
	\]

	\vspace{1em}
	\pause

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
		\textbf{Proposition 3.2} (informal): Suppose \( \calK_j - \calK_j \subset \R^d \).
		Then, there exists \( \calK_i \) for which \( \calK_i - \calK_i = \R^d \)
		and \( \calK_j \subset \calK_i \).
	\end{beamercolorbox}

	\pause
	\vspace{1em}

	\textbf{Interpretation}: if optimal \( u^*_j \neq 0 \), then set
	\[
		u_i' = u_j^* + u_i^*.
	\]
	It is possible to show this causes no problems.


\end{frame}

\begin{frame}{Cone Decompositions: Proof Sketch}

	\textbf{Proof}: Works by iteratively constructing \( \calK_i \) s.t. \( \calK_j \subset \calK_i \).

	\pause
	\horizontalrule

	We sketch a simpler statement:

	\vspace{1em}

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{relaxation}
		\textbf{Proposition 3.2} (informal): Suppose \( \calK_j = \cbr{0} \).
		Then, there exists \( \calK_i \) for which \( \calK_i - \calK_i = \R^d \)
		and \( \calK_j \subset \calK_i \).
	\end{beamercolorbox}


\end{frame}


\begin{frame}{Cone Decompositions: Proof Sketch}
	\[
		\calK_j' = \cbr{w : [S_j]_{11} \cdot \abr{x_1, w} \geq 0}
	\]
	\begin{figure}[]
		\centering
		\input{assets/empty_cone_1}
	\end{figure}
\end{frame}

\begin{frame}{Cone Decompositions: Proof Sketch}

	\[
		\calK_j'' = \calK_j' \cap \cbr{w : [S_j]_{22} \cdot \abr{x_2, w} \geq 0}
	\]

	\begin{figure}[]
		\centering
		\input{assets/empty_cone_2}
	\end{figure}
\end{frame}


\begin{frame}{Cone Decompositions: Proof Sketch}

	\[
		\calK_j''' = \calK_j'' \cap \cbr{w : [S_j]_{33} \cdot \abr{x_3, w} \geq 0}
	\]

	\begin{figure}[]
		\centering
		\input{assets/empty_cone_3}
	\end{figure}
\end{frame}


\begin{frame}{Cone Decompositions: Proof Sketch}

	\[
		\tilde \calK_j''' = \calK_j'' \cap \cbr{w : -[S_j]_{33} \cdot \abr{x_3, w} \geq 0}
	\]

	\begin{figure}[]
		\centering
		\input{assets/empty_cone_4}
	\end{figure}
\end{frame}

\begin{frame}{Cone Decomposition: Main Result}
	\begin{itemize}
		\item The real proof is more complex, but this is the core idea.
		      \vspace{0.2em}
		      \begin{itemize}
			      \item Build \( \calK_i \) by switching signs of \( [S_j]_{ii} \).
			            \vspace{0.2em}
			      \item Equivalent to turning on/off activations.
		      \end{itemize}
		      \vspace{0.4em}

		\item Leads to our main approximation result.
	\end{itemize}



	\pause
	\horizontalrule

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
		\textbf{Theorem 3.7} (informal):
		Let \( \lambda \geq 0 \) and let \( p^* \) be the optimal value of the ReLU problem.
		There exists a C-GReLU problem with minimizer \( u^* \) and optimal value \( d^* \) satisfying,
		\[
			d^* \leq p^* \leq d^* + \textcolor{Red}{2 \lambda \kappa(\tilde X_{\calJ}) \sum_{D_i \in \tilde \calD} \norm{u_i^*}_2}.
		\]
	\end{beamercolorbox}

\end{frame}

\begin{frame}{Cone Decompositions: Big Picture}
	\begin{figure}[]
		\centering
		\input{assets/relations}
	\end{figure}
	\pause

	\textbf{Takeaways}:

	\vspace{0.5em}
	\begin{itemize}
		\item Gated ReLU and ReLU model classes are the same.
		\item We can convert between them at will.
	\end{itemize}
\end{frame}


\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge V. Training Algorithms
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Algorithms: ReLU by Cone Decomposition}
	\begin{center}
		\large Using cone decompositions \textbf{in practice}.
	\end{center}

	\pause
	\horizontalrule

	\begin{enumerate}
		\item Solve the gated ReLU problem:
		      \[
			      u^* \in \argmin_{u} \norm{\sum_{j=1}^p D_j X u_j - y}_2^2 + \lambda \sum_{j=1}^p \norm{u_j}_2
		      \]
		      \pause
		\item Solve a cone decomposition:
		      \[
			      v_j^*, w_j^* \in \argmin_{v_j, w_j} \cbr{ L(v_j, w_j) : v_j - w_j = u^*_j}
		      \]
		      \pause

		\item Compute corresponding ReLU model.
	\end{enumerate}

\end{frame}

\begin{frame}{Algorithms: Solving the Convex Programs}
	We develop two algorithms for solving the convex reformulations:

	\vspace{1em}

	\begin{itemize}
		\item \textbf{R-FISTA}: a restarted FISTA variant for Gated ReLU.
		      \vspace{0.5em}
		\item \textbf{AL}: an augmented Lagrangian method for the (constrained) ReLU Problem.
	\end{itemize}

	\pause
	\horizontalrule

	And we can use all the convex tricks!
	\vspace{1em}
	\begin{itemize}
		\item \textbf{Fast}: \( O(1/T^2) \) convergence rate.
		      \vspace{0.5em}

		\item \textbf{Tuning-free}: line-search, restarts, data normalization, \ldots
		      \vspace{0.5em}

		\item \textbf{Certificates}: termination based on min-norm subgradient.
	\end{itemize}

\end{frame}

\begin{frame}{Algorithms: Completing the Picture}
	\begin{center}
		\Large
		Returning to our first example...
	\end{center}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.9\textwidth]{assets/synthetic_classification.png}
	\end{figure}
\end{frame}

\begin{frame}{Algorithms: Large-Scale Robustness}
	\begin{figure}[t]
		\centering
		\includegraphics[width=1\linewidth]{assets/pp_main.pdf}
	\end{figure}
	\begin{itemize}
		\item Generated by 438 training problems taken from UCI repo.
		\item R-FISTA/AL solve more, faster, than SGD and Adam.
	\end{itemize}
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Pause.
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Recap}
	\begin{center}
		\huge   Our Contributions.
	\end{center}

	\vspace{2em}
	\pause
	{ \large
		\begin{itemize}
			\item We leverage convex reformulations to \textbf{analyze} the set
			      of optimal ReLU networks.
			      \pause
			      \vspace{0.5em}

			\item We approximate the ReLU training problem by \textbf{unconstrained}
			      convex optimization of a Gated ReLU network.\pause
			      \vspace{0.5em}

			\item We propose and \textbf{exhaustively evaluate} algorithms for solving
			      our convex reformulations.
		\end{itemize}
	}

\end{frame}


%% main content ends %%

%% end slide
\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Try our Code!
	\end{center}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.6\textwidth]{assets/github.png}
	\end{figure}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

%% bibliography
\begin{frame}[allowframebreaks]{References}
	\printbibliography[]
\end{frame}


\end{document}
